{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Neuronal para Predicción de Precios de Arriendo\n",
    "\n",
    "En este notebook, entrenaremos un modelo de red neuronal para predecir los precios de arriendo de propiedades residenciales en los municipios del Valle de Aburrá, Antioquia. Utilizaremos los datos preprocesados que se encuentran en la carpeta `data/processed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adadelta, Nadam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Busqueda de hiperparámetros para ajustar el mejor modelo\n",
    "#se varia el número de capas ocultas, el número de neuronas por capa, la función de activación y el optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hiperparámetros optimizados con rnn\n",
    "\n",
    "\n",
    "# Cargar datos\n",
    "def load_data(filepath):\n",
    "    df = pd.read_excel(filepath)\n",
    "    return df\n",
    "\n",
    "# Procesamiento de datos\n",
    "def preprocess_data(df):\n",
    "    target = 'precio'\n",
    "    categorical_features = ['ciudad', 'antiguedad', 'comuna', 'zona', 'tipo_de_inmueble', 'estado']\n",
    "    numerical_features = df.drop(columns=categorical_features + [target]).columns.tolist()\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target].values.reshape(-1, 1)\n",
    "    target_scaler = StandardScaler()\n",
    "    y = target_scaler.fit_transform(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return preprocessor, target_scaler, X_train, X_test, y_train, y_test\n",
    "\n",
    "# Modelo de red neuronal optimizable\n",
    "def build_model(hp, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_shape,)))\n",
    "    \n",
    "    for i in range(hp.Int('num_layers', 1, 4)):\n",
    "        model.add(Dense(units=hp.Int('units_' + str(i),\n",
    "                                     min_value=32,\n",
    "                                     max_value=512,\n",
    "                                     step=32),\n",
    "                        activation=hp.Choice('activation_' + str(i), ['relu', 'tanh', 'sigmoid','leaky_relu'])))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(rate=hp.Float('dropout_' + str(i), min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    # Selección de optimizador y tasa de aprendizaje\n",
    "    optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd', 'adadelta'])\n",
    "    learning_rate = hp.Float('learning_rate', min_value=0.0001, max_value=0.02, sampling='LOG')\n",
    "\n",
    "    if optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adadelta':\n",
    "        opt = Adadelta(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Optimización de hiperparámetros\n",
    "def hyperparameter_tuning(X_train_transformed, y_train, input_shape):\n",
    "    tuner = kt.Hyperband(\n",
    "        lambda hp: build_model(hp, input_shape),\n",
    "        objective='val_mae',\n",
    "        max_epochs=20,\n",
    "        factor=2,\n",
    "        directory='hyperband',\n",
    "        project_name='nn_tuning'\n",
    "    )\n",
    "\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    tuner.search(X_train_transformed, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early], verbose=1)\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    return best_hps\n",
    "\n",
    "# Entrenamiento de la red neuronal\n",
    "def train_neural_network(X_train, X_test, y_train, y_test, preprocessor, target_scaler, best_hps):\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "    \n",
    "    model = build_model(best_hps, X_train_transformed.shape[1])\n",
    "    model.fit(X_train_transformed, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    \n",
    "    # Obtener las predicciones y reescalar la variable objetivo a su escala original\n",
    "    predictions = model.predict(X_test_transformed).flatten()\n",
    "    predictions = target_scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Evaluar el modelo\n",
    "    evaluate_model(predictions, target_scaler.inverse_transform(y_test).flatten())\n",
    "    \n",
    "    return model, predictions\n",
    "\n",
    "# Evaluación del modelo\n",
    "def evaluate_model(predictions, y_test):\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"R2 Score: {r2}\")\n",
    "    \n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.scatterplot(x=y_test, y=predictions, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Predicted Prices')\n",
    "    plt.title('Actual vs Predicted Prices')\n",
    "    plt.show()\n",
    "\n",
    "# Cargar y procesar datos\n",
    "filepath = '../data/processed/data_arriendos_model.xlsx'\n",
    "df = load_data(filepath)\n",
    "preprocessor, target_scaler, X_train, X_test, y_train, y_test = preprocess_data(df)\n",
    "\n",
    "# Transformar datos de entrenamiento para la optimización\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Optimizar hiperparámetros\n",
    "best_hps = hyperparameter_tuning(X_train_transformed, y_train, X_train_transformed.shape[1])\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(best_hps.values)\n",
    "\n",
    "# Entrenar modelo con los mejores hiperparámetros\n",
    "nn_model, nn_predictions = train_neural_network(X_train, X_test, y_train, y_test, preprocessor, target_scaler, best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mejor modelo encontrado\n",
    "\n",
    "###--------------------------------------------------###\n",
    "# Cargar datos\n",
    "def load_data(filepath):\n",
    "    df = pd.read_excel(filepath)\n",
    "    df['precio_log'] = np.log1p(df['precio'])\n",
    "    df = df.drop(columns=['precio'])\n",
    "    return df\n",
    "\n",
    "# Procesamiento de datos\n",
    "def preprocess_data(df):\n",
    "    target = 'precio_log'\n",
    "    categorical_features = ['ciudad', 'antiguedad', 'comuna', 'zona', 'tipo_de_inmueble', 'estado']\n",
    "    numerical_features = df.drop(columns=categorical_features + [target]).columns.tolist()\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target].values.reshape(-1, 1)\n",
    "    target_scaler = StandardScaler()\n",
    "    y = target_scaler.fit_transform(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return preprocessor, target_scaler, X_train, X_test, y_train, y_test\n",
    "\n",
    "# Entrenamiento de la red neuronal\n",
    "def train_neural_network(X_train, X_test, y_train, y_test, preprocessor, target_scaler):\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "    \n",
    "\n",
    "    # Definición del modelo\n",
    "    # Definición del modelo con los mejores hiperparámetros encontrados\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train_transformed.shape[1],)),\n",
    "\n",
    "        # Capa 1\n",
    "        Dense(384, activation='tanh'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "\n",
    "        # Capa 2\n",
    "        Dense(192, activation='leaky_relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        # Capa 3 (no es necesaria según los mejores hiperparámetros, pero si deseas usarla, podría ser ajustada)\n",
    "        Dense(320, activation='sigmoid'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        # Capa de salida\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compilación del modelo con el optimizador rmsprop y la tasa de aprendizaje encontrada\n",
    "    optimizer = RMSprop(learning_rate=0.00012122536697831619)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    history = model.fit(X_train_transformed, y_train, epochs=200, batch_size=64, validation_split=0.2, verbose=0)\n",
    "\n",
    "    \n",
    "    # Obtener las predicciones y reescalar la variable objetivo a su escala original\n",
    "    predictions = model.predict(X_test_transformed).flatten()\n",
    "    predictions = target_scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    y_test = target_scaler.inverse_transform(y_test).flatten()\n",
    "    y_test = np.expm1(y_test)\n",
    "    predictions = np.expm1(predictions)\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    evaluate_model(predictions, y_test)\n",
    "    \n",
    "    return model, predictions, history\n",
    "\n",
    "# Evaluación del modelo\n",
    "def evaluate_model(predictions, y_test):\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"R2 Score: {r2}\")\n",
    "    \n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.scatterplot(x=y_test, y=predictions, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "    plt.ticklabel_format(style='plain')\n",
    "    plt.xlabel('Valores reales')\n",
    "    plt.ylabel('Predicciones')\n",
    "    plt.title('Valores reales vs Predicciones')\n",
    "    plt.show()\n",
    "\n",
    "# Cargar y procesar datos\n",
    "filepath = '../data/processed/data_arriendos_model.xlsx'\n",
    "df = load_data(filepath)\n",
    "preprocessor, target_scaler, X_train, X_test, y_train, y_test = preprocess_data(df)\n",
    "\n",
    "# Entrenar modelo\n",
    "nn_model_x, nn_predictions_x, history = train_neural_network(X_train, X_test, y_train, y_test, preprocessor, target_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Funcion de perdida \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_curve(history):\n",
    "    \"\"\"\n",
    "    Genera un gráfico de la curva de pérdida (loss curve) durante el entrenamiento.\n",
    "\n",
    "    Parámetros:\n",
    "    - history: Objeto History de Keras que contiene los valores de pérdida y métrica por época.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
    "    plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida (MSE)')\n",
    "    plt.title('Curva de Pérdida (Loss Curve)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "plot_loss_curve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resumen del modelo\n",
    "nn_model_x.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuracion detallada del modelo \n",
    "nn_model_x.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pesos del modelo y variables no entrenables\n",
    "nn_model_x.get_weights()\n",
    "nn_model_x.non_trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grafico de la red neuronal obtenida \n",
    "\n",
    "# Configuración de la red\n",
    "layer_sizes = [98, 384, 192, 320, 1]  # Número de neuronas por capa\n",
    "layer_names = [\"Input\", \"Dense 1\", \"Dense 2\", \"Dense 3\", \"Output\"]\n",
    "\n",
    "# Espaciado mejorado\n",
    "x_spacing = 4  # Mayor espacio entre capas\n",
    "y_spacing = 0.3  # Más espacio entre neuronas dentro de una capa\n",
    "\n",
    "# Posiciones de las capas\n",
    "x_positions = np.arange(len(layer_sizes)) * x_spacing\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 12))\n",
    "\n",
    "# Dibujar las neuronas con mayor separación\n",
    "for i, (size, x) in enumerate(zip(layer_sizes, x_positions)):\n",
    "    y_positions = np.linspace(-size * y_spacing / 2, size * y_spacing / 2, size)\n",
    "    ax.scatter([x] * size, y_positions, s=50, c='royalblue', label=layer_names[i] if i == 0 else \"\")\n",
    "\n",
    "    # Dibujar conexiones con más separación\n",
    "    if i > 0:\n",
    "        prev_size = layer_sizes[i - 1]\n",
    "        prev_y_positions = np.linspace(-prev_size * y_spacing / 2, prev_size * y_spacing / 2, prev_size)\n",
    "        for y1 in prev_y_positions:\n",
    "            for y2 in y_positions:\n",
    "                ax.plot([x_positions[i - 1], x], [y1, y2], c=\"lightgray\", lw=0.3, alpha=0.7)\n",
    "\n",
    "# Etiquetas de capas\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels(layer_names, fontsize=12)\n",
    "ax.set_yticks([])  # Ocultar el eje Y\n",
    "ax.set_title(\"Estructura de la Red Neuronal (Matplotlib Mejorado)\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".ipynb",
   "mimetype": "application/x-ipynb+json",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
